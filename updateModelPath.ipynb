{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUOwSP/raJ8OgaSxkSCrns",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eportah/Bird-of-Prey-Identifier/blob/main/updateModelPath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup dataset\n",
        "\n",
        "#mount the drive to retrieve dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#import ZipFile to read dataset\n",
        "from zipfile import ZipFile\n",
        "\n",
        "#establish paths\n",
        "zipPath = \"/content/drive/MyDrive/MachineLearning/Datasets/birdsDatasets.zip\"\n",
        "extractPath = \"/content/extractPath\"\n",
        "\n",
        "#extract and open dataset in read mode using ZipFile\n",
        "with ZipFile(zipPath, 'r') as zipObj:\n",
        "   zipObj.extractall(extractPath)"
      ],
      "metadata": {
        "id": "Risny0ZVTNNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b6c7bf-afb8-4e0f-e91f-255f37e76a65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "- Dataset has 300 files (images) split into 2 classes.\n",
        "- Class names are 'birdsOfPrey' and 'notBirdsOfPrey'.\n",
        "- Images are batched into groups of 32, resized to 224x224 pixels, with 3 color channels RGB.\n",
        "- Labels are stored in 1-D array of length 32, one label per image in the batch.\n",
        "\"\"\"\n",
        "\n",
        "#store dataset path into variable and import tensorflow to load images from directory\n",
        "datasetDirectory = \"/content/extractPath/birdsDataset\"\n",
        "import tensorflow as tf\n",
        "\n",
        "#load dataset with function image_dataset_from_directory and establish both image size and batch size\n",
        "trainDataset = tf.keras.utils.image_dataset_from_directory(datasetDirectory, image_size=(224,224), batch_size=32)\n",
        "\n",
        "#preview what got loaded and what dataset looks like\n",
        "class_names = trainDataset.class_names\n",
        "print(\"Class names:\", class_names)\n",
        "for images, labels in trainDataset.take(1):\n",
        "  print(\"Image batch shape:\", images.shape)\n",
        "  print(\"Label batch shape:\", labels.shape)"
      ],
      "metadata": {
        "id": "gNYqXl4QhUxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split dataset\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "- 210, 70% for training\n",
        "- 90, 30% for validation\n",
        "- Batch sizes are the same for both training and validation\n",
        "\"\"\"\n",
        "\n",
        "#load training dataset, 70% of data\n",
        "trainingDataset = tf.keras.utils.image_dataset_from_directory(\n",
        "   datasetDirectory,\n",
        "   validation_split=0.3,\n",
        "   subset=\"training\",\n",
        "   seed=123,\n",
        "   image_size=(224,224),\n",
        "   batch_size=32\n",
        ")\n",
        "\n",
        "#load validation dataset, 30% of data\n",
        "validationDataset = tf.keras.utils.image_dataset_from_directory(\n",
        "   datasetDirectory,\n",
        "   validation_split=0.3,\n",
        "   subset=\"validation\",\n",
        "   seed=123,\n",
        "   image_size=(224,224),\n",
        "   batch_size=32\n",
        ")\n",
        "\n",
        "#preview one batch from training dataset\n",
        "print(\"Training batch shapes:\")\n",
        "for imagesTrain, labelsTrain in trainingDataset.take(1):\n",
        "  print(\"Images:\", imagesTrain.shape)\n",
        "  print(\"Labels:\", labelsTrain.shape)\n",
        "\n",
        "#preview one batch from validation dataset\n",
        "print(\"Validation batch shapes:\")\n",
        "for imagesVal, labelsVal in validationDataset.take(1):\n",
        " print(\"Images:\", imagesVal.shape)\n",
        " print(\"Labels:\", labelsVal.shape)"
      ],
      "metadata": {
        "id": "34j1xs9XhiJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess dataset\n",
        "\n",
        "#import MobileNetV2 preprocessing\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "#define dataAugmentation\n",
        "dataAugmentation = tf.keras.Sequential([\n",
        "   tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "   tf.keras.layers.RandomRotation(0.1),\n",
        "   tf.keras.layers.RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "#apply augmentation and MobileNetV2 preprocessing to training set using map to transform batches of images\n",
        "trainSet = trainingDataset.map(lambda x, y: (preprocess_input(dataAugmentation(x)),y))\n",
        "\n",
        "#apply MobileNetV2 preprocessing to validation set still using map to transform batches of images\n",
        "validSet = validationDataset.map(lambda x, y: (preprocess_input(x),y))"
      ],
      "metadata": {
        "id": "iUVKpUhCiC4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview augmentation\n",
        "\n",
        "#import pyplot to visualize tensors as images and numpy for numpy indexing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#take one raw image from unmapped training dataset and select first image and its label, shape (224, 224, 3)\n",
        "for imgs, labs in trainingDataset.take(1):\n",
        "  batchImages = imgs.numpy()\n",
        "  batchLabels = labs.numpy()\n",
        "sampleImage = batchImages[0]\n",
        "sampleLabel = batchLabels[0]\n",
        "\n",
        "#establish grid and create a figure for plotting\n",
        "augmentedNum = 9\n",
        "plt.figure(figsize = (6,6))\n",
        "\n",
        "#add batch dimension and apply augmentation\n",
        "for i in range(augmentedNum):\n",
        "  imgBatch = np.expand_dims(sampleImage, axis = 0)\n",
        "  imgAugmented = dataAugmentation(imgBatch)[0].numpy()\n",
        "\n",
        "  #clip to ensure values are within range and cast to convert the aray from floats to 8-bit unsigned integers\n",
        "  imgAugmented = np.clip(imgAugmented, 0, 255).astype(\"uint8\")\n",
        "\n",
        "  #finish off by plotting each augmented image in a 3x3 grid\n",
        "  axes = plt.subplot(3,3, i+1)\n",
        "  axes.imshow(imgAugmented)\n",
        "  axes.axis('off')"
      ],
      "metadata": {
        "id": "nFi5j0mlKzJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "\n",
        "#import MobileNetV2 to use as base\n",
        "from keras.applications import MobileNetV2\n",
        "from keras import layers, models\n",
        "\n",
        "#load base MobiletNetV2 and drop original classifier\n",
        "baseModel = MobileNetV2(\n",
        "    input_shape = (224, 224, 3),\n",
        "    include_top = False,\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "#freeze base so pretrained weights are not trained right away\n",
        "baseModel.trainable = False\n",
        "\n",
        "#add custom classifier head by using layers\n",
        "\"\"\"\n",
        "Rectified linear unit relu to prevent exploding values\n",
        "Softmax to turn output into probabilites that add up to 1, each value shows chances an image could belong to a class\n",
        "Dense layer to learn how much each feature matters for classying a species with another\n",
        "GAP2D to squeeze feature maps into single vector per channel instead of flattening huge tensors\n",
        "Dropout to randomly turn off 30% neurons during training to prevent overfitting\n",
        "\"\"\"\n",
        "model = models.Sequential([\n",
        "    baseModel,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(6, activation = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "oqjvEYwkPkhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile model\n",
        "\n",
        "#import optimizers to use Adam\n",
        "from keras import optimizers\n",
        "\n",
        "#.compile for optimizer, loss, metrics\n",
        "\"\"\"\n",
        "Set learning rate to 0.0001 to avoid ruining pretrained weights\n",
        "Sparse categorical crossentropy since classes are labeled as integers\n",
        "Accuracy metric to track model performance\n",
        "\"\"\"\n",
        "model.compile(\n",
        "    optimizer = optimizers.Adam(learning_rate = 0.0001),\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "9U1XNtx_UfdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "Training data divided into batches of images (batch size = 32)\n",
        "7 batches per epoch\n",
        "Model updates its weights after each batch\n",
        "Over epochs, accuracy increases and loss decreases\n",
        "Validation accuracy starts high due to pretrained MobileNetV2 base\n",
        "\"\"\"\n",
        "\n",
        "#set 15 epochs to get started and change depending on how it fits\n",
        "epochs = 15\n",
        "\n",
        "#fit the model with training history\n",
        "\"\"\"\n",
        "history stores loss and accuracy curves to be plotted later\n",
        "\"\"\"\n",
        "history = model.fit(\n",
        "    trainSet,\n",
        "    validation_data = validSet,\n",
        "    epochs = epochs\n",
        ")"
      ],
      "metadata": {
        "id": "w-eY7CU9kBvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate curves\n",
        "\n",
        "#extract metrics from history\n",
        "acc = history.history['accuracy']\n",
        "valAcc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "valLoss = history.history['val_loss']\n",
        "\n",
        "#number of epochs\n",
        "epochsRange = range(len(acc))\n",
        "\n",
        "#plot accuracy and loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#accuracy subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochsRange, acc, label = 'Training Accuracy')\n",
        "plt.plot(epochsRange, valAcc, label = 'Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "#loss subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochsRange, loss, label = 'Training Loss')\n",
        "plt.plot(epochsRange, valLoss, label = 'Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DGZmCPS0tYwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect predictions\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "3x3 grid of 9 birds\n",
        "Prediction compared with ground truth\n",
        "If correctly predicted, values (1 or 0) align\n",
        "birdsOfPrey, 1\n",
        "notBirdsOfPrey, 0\n",
        "\"\"\"\n",
        "\n",
        "#take batch from validation set\n",
        "\"\"\"\n",
        ".predict to get probabilities\n",
        ".argmax to get predicted class\n",
        "axis=1 to look across columns for each row\n",
        "actual (true) labels is ground truth\n",
        "\"\"\"\n",
        "for images, labels in validSet.take(1):\n",
        "  predictions = model.predict(images)\n",
        "  predictedLabels = np.argmax(predictions, axis=1)\n",
        "  actualLabels = labels.numpy()\n",
        "  imagesBatch = images.numpy()\n",
        "\n",
        "#flip labels so that birdsOfPrey = 1\n",
        "predictedLabels = 1 - predictedLabels\n",
        "actualLabels = 1- actualLabels\n",
        "\n",
        "#plot first 9 images in batch with prediction vs ground truth\n",
        "\"\"\"\n",
        "Show first 9 images in batch for simplicity\n",
        "(images_batch[i]+1)/2 to convert from [-1,1] to [0,1]\n",
        "Fstring to format labels\n",
        "True for ground truth\n",
        ".axis off to hide unwanted ticks and labels\n",
        "\"\"\"\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(9):\n",
        "  axes = plt.subplot(3, 3, i+1)\n",
        "  plt.imshow((imagesBatch[i]+1)/2)\n",
        "  plt.title(f\"Prediction: {predictedLabels[i]}, True: {actualLabels[i]}\")\n",
        "  plt.axis('off')\n",
        "\n",
        "#tight layout so nothing overlaps\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yFj2WXGj5PRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "\n",
        "#save to birdOfPreyIdentifier.keras\n",
        "\"\"\"\n",
        ".save to save completed model into permanent file\n",
        ".keras as the file extension to save architecture, weights, and optimization state\n",
        "File saved to temporary storage of Colab session\n",
        "Saved permanently by downloading to desktop and uploading to mounted drive\n",
        "\"\"\"\n",
        "model.save('birdOfPreyIdentifier.keras')\n",
        "\n",
        "#print statement to show save was succesful\n",
        "print(\"Model saved to birdOfPreyIdentifier.keras\")"
      ],
      "metadata": {
        "id": "w8AitiSbsWN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "Layers\n",
        "MobileNetV2 (base) extracts general image features\n",
        "Custom layers take features from the base and learn how to use them for bird images\n",
        "\n",
        "Output Shape\n",
        "'None' represents flexible batch size, allows for training on large batches and predictions on individual items\n",
        "Last shape means model outputs 6 prob scores for each image, one for each of the 6 bird classes\n",
        "\n",
        "Parameters\n",
        "Individual dials the model adjusts during training to learn\n",
        "Param of 0 because they just perform a fixed math operation without learning anything\n",
        "Trainable params are what was specifically tuned in project for bird dataset\n",
        "Non-trainable params are frozen dials from pre-trained MobileNetV2 base that is being borrowed\n",
        "Optimizer params are internal variables Adam uses to keep track of training process, not part of model's layers\n",
        "Adam keeps track of its past average gradients and past average of the square of its gradients\n",
        "\"\"\"\n",
        "\n",
        "#import tf to make cell self-contained and avoid running extra cells\n",
        "import tensorflow as tf\n",
        "\n",
        "#establish path\n",
        "modelPath = '/content/drive/MyDrive/MachineLearning/Models/birdOfPreyIdentifier.keras'\n",
        "\n",
        "#load trained model\n",
        "loadedModel = tf.keras.models.load_model(modelPath)\n",
        "\n",
        "#print model summary to show load was succesful along with additional model info\n",
        "loadedModel.summary()"
      ],
      "metadata": {
        "id": "gisO1ndzCZWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "0d287f38-d068-457c-947a-40a59291cd43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,752,212\u001b[0m (10.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,752,212</span> (10.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,742\u001b[0m (643.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,742</span> (643.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m329,486\u001b[0m (1.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">329,486</span> (1.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict unseen data\n",
        "\n",
        "#prep for image preprocessing\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "#define class names used during training and define prediction function\n",
        "\"\"\"\n",
        "Load image from file path while resizing to match input size model was trained on\n",
        "Convert image to np array to allow model to work with numbers rather than image files\n",
        "Not using np.asarray() as it will be in uint8 rather than desired float32 with img_to_array\n",
        "Modify shape by adding extra dimension to turn single image into batch of one\n",
        "Set axis = 0 to insert new dimension at the very front of original shape (224,224,3)\n",
        "Preprocess image by applying same scaling used during training\n",
        "Get model's prediction to feed prepped image to loaded model and output array of probabilities\n",
        "\"\"\"\n",
        "classNames = ['birdsOfPrey', 'notBirdsOfPrey']\n",
        "def predictBirdOfPrey(imagePath, model):\n",
        "\n",
        "  img = image.load_img(imagePath, target_size = (224, 224))\n",
        "  imgArray = image.img_to_array(img)\n",
        "  imgBatch = np.expand_dims(imgArray, axis = 0)\n",
        "  imgPreprocess = preprocess_input(imgBatch)\n",
        "  prediction = model.predict(imgPreprocess)\n",
        "\n",
        "  #establish prediction's human readibility and return for later use\n",
        "  \"\"\"\n",
        "  Uses np.argmax to find index of highest probability\n",
        "  Uses predicted index to look up the name in list\n",
        "  Uses np.max to get highest probability and convert to percentage for confidence score\n",
        "  Confidence score is probability that model assigns to its prediction for a single image\n",
        "  \"\"\"\n",
        "  indexPredicted = np.argmax(prediction[0])\n",
        "  classPredicted = classNames[indexPredicted]\n",
        "  confidence = np.max(prediction[0]) * 100\n",
        "  return classPredicted, confidence\n",
        "\n",
        "#test function and use it to make prediction and print results with f-string\n",
        "testImagePath = '/content/extractPath/birdsDataset/birdsOfPrey/eagle/eagleFlight1.jpg'\n",
        "classPredicted, confidence = predictBirdOfPrey(testImagePath, loadedModel)\n",
        "print(f\"Prediction: {classPredicted}\")\n",
        "print(f\"Confidence: {confidence:.2f}%\")"
      ],
      "metadata": {
        "id": "N7vkXX2kkF7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931ef7cd-4ac7-45ee-9bc7-0eca44bf35bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Prediction: birdsOfPrey\n",
            "Confidence: 99.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install StreamLit by getting library\n",
        "\n",
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "PcfG9vhDH4Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7bce1b-a6a2-42c8-a762-d387c9146e41"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create app file\n",
        "\n",
        "#import needed libraries and save cell content to new file called birdOfPreyApp.py\n",
        "%%writefile birdOfPreyApp.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "#set up page\n",
        "st.set_page_config(pageTitle=\"Bird Of Prey Identifier\")\n",
        "st.title(\"Bird of Prey Identifier\")\n",
        "st.write(\"Upload an image of a bird for the model to predict if it's a bird of prey or not.\")\n",
        "\n",
        "#streamlit cache command so app loads model once rather than each time an image is uploaded\n",
        "@st.cache.resource\n",
        "\n",
        "#load saved model and define prediction function\n",
        "MODEL_PATH = 'birdOfPreyIdentifier.keras'\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "def loadTrainedModel():\n",
        "  model = tf.keras.models.load_model(MODEL_PATH)\n",
        "  return model\n",
        "\n",
        "#define class names\n",
        "CLASS_NAMES = ['eagle', 'falcon', 'hawk', 'owl', 'vulture', 'NotBirdsOfPrey']\n",
        "\n",
        "#preprocess image and return predicted class and confidence score\n",
        "\"\"\"\n",
        "convert image to numpy array\n",
        "add extra dimension for batch\n",
        "preprocess image for model\n",
        "get model's prediction and turn into human-readable result\n",
        "get specific bird type\n",
        "\"\"\"\n",
        "def predictBirdOfPrey(img_to_predict, model):\n",
        "  img = img_to_predict.convert('RGB')\n",
        "  img = img.resize((224, 224))\n",
        "  img_array = image.img_to_array(img)\n",
        "\n",
        "  img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "  img_preprocessed = preprocess_input(img_batch)\n",
        "\n",
        "  prediction = model.predict(img_preprocessed)\n",
        "  predictedIndex = np.argmax(prediction[0])\n",
        "  specificBird = CLASS_NAMES[predictedIndex]\n",
        "  confidence = np.max(prediction[0]) * 100\n",
        "\n",
        "  if specificBird == 'notBirdOfPrey':\n",
        "    resultSpecBird = \"Not a bird of prey\"\n",
        "  else:\n",
        "    resultSpecBird = f\"Bird of Prey ({specificBird.capitalize()})\"\n",
        "\n",
        "  return resultSpecBird, confidence\n",
        "\n",
        "#load model and create file uploader\n",
        "model = loadTrainedModel()\n",
        "uploadedFile = st.file_uploader(\"Upload an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "#classify uploaded image and yield prediction and confidence\n",
        "\"\"\"\n",
        "st.image to open file as an image\n",
        "st.button to create Classify button\n",
        "st.spinner for a loading message\n",
        "st.success and st.info for results\n",
        "\"\"\"\n",
        "if uploadedFile is not None:\n",
        "  pil_image = Image.open(uploadedFile)\n",
        "  st.image(pil_image, caption='Uploaded image', use_column_width=True)\n",
        "  if st.button('Classify'):\n",
        "    with st.spinner('Classifying...'):\n",
        "      predictedClass, confidence = predictBirdOfPrey(pil_image, model)\n",
        "      st.success(f\"Prediction: **{predictedClass}**\")\n",
        "      st.info(f\"Confidence: **{confidence:.2f}%**\")"
      ],
      "metadata": {
        "id": "manGpLHYIG_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af38ce1b-d237-48c1-8943-19a7cfdfcaa9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting birdOfPreyApp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deployment requirements\n",
        "\n",
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "tensorflow\n",
        "Pillow\n",
        "numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcxKo6qjofzo",
        "outputId": "01bf984b-3836-4b66-8256-337c4b282eec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    }
  ]
}