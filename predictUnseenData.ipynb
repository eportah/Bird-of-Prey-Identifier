{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKUdKQji1x9R0Pdo28yy23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eportah/Bird-of-Prey-Identifier/blob/main/predictUnseenData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup dataset\n",
        "\n",
        "#mount the drive to retrieve dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#import ZipFile to read dataset\n",
        "from zipfile import ZipFile\n",
        "\n",
        "#establish paths\n",
        "zipPath = \"/content/drive/MyDrive/MachineLearning/Datasets/birdsDatasets.zip\"\n",
        "extractPath = \"/content/extractPath\"\n",
        "\n",
        "#extract and open dataset in read mode using ZipFile\n",
        "with ZipFile(zipPath, 'r') as zipObj:\n",
        "   zipObj.extractall(extractPath)"
      ],
      "metadata": {
        "id": "Risny0ZVTNNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4b406f-d4fc-4157-fcbb-4dd28e9bbea9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "- Dataset has 300 files (images) split into 2 classes.\n",
        "- Class names are 'birdsOfPrey' and 'notBirdsOfPrey'.\n",
        "- Images are batched into groups of 32, resized to 224x224 pixels, with 3 color channels RGB.\n",
        "- Labels are stored in 1-D array of length 32, one label per image in the batch.\n",
        "\"\"\"\n",
        "\n",
        "#store dataset path into variable and import tensorflow to load images from directory\n",
        "datasetDirectory = \"/content/extractPath/birdsDataset\"\n",
        "import tensorflow as tf\n",
        "\n",
        "#load dataset with function image_dataset_from_directory and establish both image size and batch size\n",
        "trainDataset = tf.keras.utils.image_dataset_from_directory(datasetDirectory, image_size=(224,224), batch_size=32)\n",
        "\n",
        "#preview what got loaded and what dataset looks like\n",
        "class_names = trainDataset.class_names\n",
        "print(\"Class names:\", class_names)\n",
        "for images, labels in trainDataset.take(1):\n",
        "  print(\"Image batch shape:\", images.shape)\n",
        "  print(\"Label batch shape:\", labels.shape)"
      ],
      "metadata": {
        "id": "gNYqXl4QhUxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split dataset\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "- 210, 70% for training\n",
        "- 90, 30% for validation\n",
        "- Batch sizes are the same for both training and validation\n",
        "\"\"\"\n",
        "\n",
        "#load training dataset, 70% of data\n",
        "trainingDataset = tf.keras.utils.image_dataset_from_directory(\n",
        "   datasetDirectory,\n",
        "   validation_split=0.3,\n",
        "   subset=\"training\",\n",
        "   seed=123,\n",
        "   image_size=(224,224),\n",
        "   batch_size=32\n",
        ")\n",
        "\n",
        "#load validation dataset, 30% of data\n",
        "validationDataset = tf.keras.utils.image_dataset_from_directory(\n",
        "   datasetDirectory,\n",
        "   validation_split=0.3,\n",
        "   subset=\"validation\",\n",
        "   seed=123,\n",
        "   image_size=(224,224),\n",
        "   batch_size=32\n",
        ")\n",
        "\n",
        "#preview one batch from training dataset\n",
        "print(\"Training batch shapes:\")\n",
        "for imagesTrain, labelsTrain in trainingDataset.take(1):\n",
        "  print(\"Images:\", imagesTrain.shape)\n",
        "  print(\"Labels:\", labelsTrain.shape)\n",
        "\n",
        "#preview one batch from validation dataset\n",
        "print(\"Validation batch shapes:\")\n",
        "for imagesVal, labelsVal in validationDataset.take(1):\n",
        " print(\"Images:\", imagesVal.shape)\n",
        " print(\"Labels:\", labelsVal.shape)"
      ],
      "metadata": {
        "id": "34j1xs9XhiJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess dataset\n",
        "\n",
        "#import MobileNetV2 preprocessing\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "#define dataAugmentation\n",
        "dataAugmentation = tf.keras.Sequential([\n",
        "   tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "   tf.keras.layers.RandomRotation(0.1),\n",
        "   tf.keras.layers.RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "#apply augmentation and MobileNetV2 preprocessing to training set using map to transform batches of images\n",
        "trainSet = trainingDataset.map(lambda x, y: (preprocess_input(dataAugmentation(x)),y))\n",
        "\n",
        "#apply MobileNetV2 preprocessing to validation set still using map to transform batches of images\n",
        "validSet = validationDataset.map(lambda x, y: (preprocess_input(x),y))"
      ],
      "metadata": {
        "id": "iUVKpUhCiC4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview augmentation\n",
        "\n",
        "#import pyplot to visualize tensors as images and numpy for numpy indexing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#take one raw image from unmapped training dataset and select first image and its label, shape (224, 224, 3)\n",
        "for imgs, labs in trainingDataset.take(1):\n",
        "  batchImages = imgs.numpy()\n",
        "  batchLabels = labs.numpy()\n",
        "sampleImage = batchImages[0]\n",
        "sampleLabel = batchLabels[0]\n",
        "\n",
        "#establish grid and create a figure for plotting\n",
        "augmentedNum = 9\n",
        "plt.figure(figsize = (6,6))\n",
        "\n",
        "#add batch dimension and apply augmentation\n",
        "for i in range(augmentedNum):\n",
        "  imgBatch = np.expand_dims(sampleImage, axis = 0)\n",
        "  imgAugmented = dataAugmentation(imgBatch)[0].numpy()\n",
        "\n",
        "  #clip to ensure values are within range and cast to convert the aray from floats to 8-bit unsigned integers\n",
        "  imgAugmented = np.clip(imgAugmented, 0, 255).astype(\"uint8\")\n",
        "\n",
        "  #finish off by plotting each augmented image in a 3x3 grid\n",
        "  axes = plt.subplot(3,3, i+1)\n",
        "  axes.imshow(imgAugmented)\n",
        "  axes.axis('off')"
      ],
      "metadata": {
        "id": "nFi5j0mlKzJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "\n",
        "#import MobileNetV2 to use as base\n",
        "from keras.applications import MobileNetV2\n",
        "from keras import layers, models\n",
        "\n",
        "#load base MobiletNetV2 and drop original classifier\n",
        "baseModel = MobileNetV2(\n",
        "    input_shape = (224, 224, 3),\n",
        "    include_top = False,\n",
        "    weights = 'imagenet'\n",
        ")\n",
        "\n",
        "#freeze base so pretrained weights are not trained right away\n",
        "baseModel.trainable = False\n",
        "\n",
        "#add custom classifier head by using layers\n",
        "\"\"\"\n",
        "Rectified linear unit relu to prevent exploding values\n",
        "Softmax to turn output into probabilites that add up to 1, each value shows chances an image could belong to a class\n",
        "Dense layer to learn how much each feature matters for classying a species with another\n",
        "GAP2D to squeeze feature maps into single vector per channel instead of flattening huge tensors\n",
        "Dropout to randomly turn off 30% neurons during training to prevent overfitting\n",
        "\"\"\"\n",
        "model = models.Sequential([\n",
        "    baseModel,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(6, activation = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "oqjvEYwkPkhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile model\n",
        "\n",
        "#import optimizers to use Adam\n",
        "from keras import optimizers\n",
        "\n",
        "#.compile for optimizer, loss, metrics\n",
        "\"\"\"\n",
        "Set learning rate to 0.0001 to avoid ruining pretrained weights\n",
        "Sparse categorical crossentropy since classes are labeled as integers\n",
        "Accuracy metric to track model performance\n",
        "\"\"\"\n",
        "model.compile(\n",
        "    optimizer = optimizers.Adam(learning_rate = 0.0001),\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "9U1XNtx_UfdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "Training data divided into batches of images (batch size = 32)\n",
        "7 batches per epoch\n",
        "Model updates its weights after each batch\n",
        "Over epochs, accuracy increases and loss decreases\n",
        "Validation accuracy starts high due to pretrained MobileNetV2 base\n",
        "\"\"\"\n",
        "\n",
        "#set 15 epochs to get started and change depending on how it fits\n",
        "epochs = 15\n",
        "\n",
        "#fit the model with training history\n",
        "\"\"\"\n",
        "history stores loss and accuracy curves to be plotted later\n",
        "\"\"\"\n",
        "history = model.fit(\n",
        "    trainSet,\n",
        "    validation_data = validSet,\n",
        "    epochs = epochs\n",
        ")"
      ],
      "metadata": {
        "id": "w-eY7CU9kBvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate curves\n",
        "\n",
        "#extract metrics from history\n",
        "acc = history.history['accuracy']\n",
        "valAcc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "valLoss = history.history['val_loss']\n",
        "\n",
        "#number of epochs\n",
        "epochsRange = range(len(acc))\n",
        "\n",
        "#plot accuracy and loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#accuracy subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochsRange, acc, label = 'Training Accuracy')\n",
        "plt.plot(epochsRange, valAcc, label = 'Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "#loss subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochsRange, loss, label = 'Training Loss')\n",
        "plt.plot(epochsRange, valLoss, label = 'Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DGZmCPS0tYwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect predictions\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "3x3 grid of 9 birds\n",
        "Prediction compared with ground truth\n",
        "If correctly predicted, values (1 or 0) align\n",
        "birdsOfPrey, 1\n",
        "notBirdsOfPrey, 0\n",
        "\"\"\"\n",
        "\n",
        "#take batch from validation set\n",
        "\"\"\"\n",
        ".predict to get probabilities\n",
        ".argmax to get predicted class\n",
        "axis=1 to look across columns for each row\n",
        "actual (true) labels is ground truth\n",
        "\"\"\"\n",
        "for images, labels in validSet.take(1):\n",
        "  predictions = model.predict(images)\n",
        "  predictedLabels = np.argmax(predictions, axis=1)\n",
        "  actualLabels = labels.numpy()\n",
        "  imagesBatch = images.numpy()\n",
        "\n",
        "#flip labels so that birdsOfPrey = 1\n",
        "predictedLabels = 1 - predictedLabels\n",
        "actualLabels = 1- actualLabels\n",
        "\n",
        "#plot first 9 images in batch with prediction vs ground truth\n",
        "\"\"\"\n",
        "Show first 9 images in batch for simplicity\n",
        "(images_batch[i]+1)/2 to convert from [-1,1] to [0,1]\n",
        "Fstring to format labels\n",
        "True for ground truth\n",
        ".axis off to hide unwanted ticks and labels\n",
        "\"\"\"\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(9):\n",
        "  axes = plt.subplot(3, 3, i+1)\n",
        "  plt.imshow((imagesBatch[i]+1)/2)\n",
        "  plt.title(f\"Prediction: {predictedLabels[i]}, True: {actualLabels[i]}\")\n",
        "  plt.axis('off')\n",
        "\n",
        "#tight layout so nothing overlaps\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yFj2WXGj5PRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "\n",
        "#save to birdOfPreyIdentifier.keras\n",
        "\"\"\"\n",
        ".save to save completed model into permanent file\n",
        ".keras as the file extension to save architecture, weights, and optimization state\n",
        "File saved to temporary storage of Colab session\n",
        "Saved permanently by downloading to desktop and uploading to mounted drive\n",
        "\"\"\"\n",
        "model.save('birdOfPreyIdentifier.keras')\n",
        "\n",
        "#print statement to show save was succesful\n",
        "print(\"Model saved to birdOfPreyIdentifier.keras\")"
      ],
      "metadata": {
        "id": "w8AitiSbsWN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "\n",
        "#cell output rundown\n",
        "\"\"\"\n",
        "Layers\n",
        "MobileNetV2 (base) extracts general image features\n",
        "Custom layers take features from the base and learn how to use them for bird images\n",
        "\n",
        "Output Shape\n",
        "'None' represents flexible batch size, allows for training on large batches and predictions on individual items\n",
        "Last shape means model outputs 6 prob scores for each image, one for each of the 6 bird classes\n",
        "\n",
        "Parameters\n",
        "Individual dials the model adjusts during training to learn\n",
        "Param of 0 because they just perform a fixed math operation without learning anything\n",
        "Trainable params are what was specifically tuned in project for bird dataset\n",
        "Non-trainable params are frozen dials from pre-trained MobileNetV2 base that is being borrowed\n",
        "Optimizer params are internal variables Adam uses to keep track of training process, not part of model's layers\n",
        "Adam keeps track of its past average gradients and past average of the square of its gradients\n",
        "\"\"\"\n",
        "\n",
        "#import tf to make cell self-contained and avoid running extra cells\n",
        "import tensorflow as tf\n",
        "\n",
        "#establish path\n",
        "modelPath = '/content/drive/MyDrive/MachineLearning/Models/birdOfPreyIdentifier.keras'\n",
        "\n",
        "#load trained model\n",
        "loadedModel = tf.keras.models.load_model(modelPath)\n",
        "\n",
        "#print model summary to show load was succesful along with additional model info\n",
        "loadedModel.summary()"
      ],
      "metadata": {
        "id": "gisO1ndzCZWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "fdae3190-67ab-46a6-8a2c-eaf18ba96b52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,752,212\u001b[0m (10.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,752,212</span> (10.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,742\u001b[0m (643.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,742</span> (643.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m329,486\u001b[0m (1.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">329,486</span> (1.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict unseen data\n",
        "\n",
        "#prep for image preprocessing\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "#define class names used during training and define prediction function\n",
        "\"\"\"\n",
        "Load image from file path while resizing to match input size model was trained on\n",
        "Convert image to np array to allow model to work with numbers rather than image files\n",
        "Not using np.asarray() as it will be in uint8 rather than desired float32 with img_to_array\n",
        "Modify shape by adding extra dimension to turn single image into batch of one\n",
        "Set axis = 0 to insert new dimension at the very front of original shape (224,224,3)\n",
        "Preprocess image by applying same scaling used during training\n",
        "Get model's prediction to feed prepped image to loaded model and output array of probabilities\n",
        "\"\"\"\n",
        "classNames = ['birdsOfPrey', 'notBirdsOfPrey']\n",
        "def predictBirdOfPrey(imagePath, model):\n",
        "\n",
        "  img = image.load_img(imagePath, target_size = (224, 224))\n",
        "  imgArray = image.img_to_array(img)\n",
        "  imgBatch = np.expand_dims(imgArray, axis = 0)\n",
        "  imgPreprocess = preprocess_input(imgBatch)\n",
        "  prediction = model.predict(imgPreprocess)\n",
        "\n",
        "  #establish prediction's human readibility and return for later use\n",
        "  \"\"\"\n",
        "  Uses np.argmax to find index of highest probability\n",
        "  Uses predicted index to look up the name in list\n",
        "  Uses np.max to get highest probability and convert to percentage for confidence score\n",
        "  Confidence score is probability that model assigns to its prediction for a single image\n",
        "  \"\"\"\n",
        "  indexPredicted = np.argmax(prediction[0])\n",
        "  classPredicted = classNames[indexPredicted]\n",
        "  confidence = np.max(prediction[0]) * 100\n",
        "  return classPredicted, confidence\n",
        "\n",
        "#test function and use it to make prediction and print results with f-string\n",
        "testImagePath = '/content/extractPath/birdsDataset/birdsOfPrey/eagle/eagleFlight1.jpg'\n",
        "classPredicted, confidence = predictBirdOfPrey(testImagePath, loadedModel)\n",
        "print(f\"Prediction: {classPredicted}\")\n",
        "print(f\"Confidence: {confidence:.2f}%\")"
      ],
      "metadata": {
        "id": "N7vkXX2kkF7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931ef7cd-4ac7-45ee-9bc7-0eca44bf35bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Prediction: birdsOfPrey\n",
            "Confidence: 99.55%\n"
          ]
        }
      ]
    }
  ]
}